{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Twitter Sentiment Analysis ",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Twitter Sentiment Analysis\n",
        "\n",
        "Aranged by StudentID-StudentName:\n",
        "\n",
        "12S18033-Cristina Sriwahyuni Hasibuan\n",
        "\n",
        "12S18038-Naomi A. Simatupang\n",
        "\n",
        "12S18049-Natasya Sitorus\n",
        "\n",
        "12S18060-Elsaday Bakara"
      ],
      "metadata": {
        "id": "K3-RVRXZ5fve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Requirements installation if not exist yet"
      ],
      "metadata": {
        "id": "46dk25eHmhJ0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Xi-NhSE8w5wh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db47398a-e032-4efb-abe4-a593309345c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Requirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.3)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweet-preprocessor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtlL7YaXoEsU",
        "outputId": "78260791-34de-450e-e2fd-e4a8677eed37"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tweet-preprocessor\n",
            "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import Lib"
      ],
      "metadata": {
        "id": "ZxpwCA3P-npR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "import json\n",
        "import csv\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import tweepy\n",
        "from tweepy import OAuthHandler\n",
        "from tweepy import Stream\n",
        "from collections import Counter\n",
        "from plotly import graph_objs as go\n",
        "import plotly.express as px\n",
        "import plotly.figure_factory as ff\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.ml.feature import Tokenizer, StringIndexer, Word2Vec, StopWordsRemover, HashingTF\n",
        "from pyspark.ml import Pipeline, Transformer\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics"
      ],
      "metadata": {
        "id": "cFjFe3t1Gl3Y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from datetime import timedelta, datetime\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import matplotlib.pyplot as plt \n",
        "from tweepy import OAuthHandler\n",
        "from textblob import TextBlob\n",
        "import preprocessor as p\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tweepy\n",
        "import csv\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqi5MUu0nypi",
        "outputId": "e67cf948-e529-4256-fa8a-de38cd893bfc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Configuration"
      ],
      "metadata": {
        "id": "NCqJdwHUmVRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "APIKey = \"XEu6VXcA06Aneo8y0JYiZoC34\"\n",
        "APISecretKey = \"lKRAifMEA6GOIPRzsGDLQ4RKNBUT9EfAjTFRiztmVzdFBmRwln\"\n",
        "AccessToken = \"1390698546353999878-65J1HL6hFuH2BSZyMsDOn5xJiIIl1C\"\n",
        "AccessTokenSecret = \"HvMZfhKfXQv0NFiWRmdXRl27cYfsHpQsImw0okSuyQpvg\""
      ],
      "metadata": {
        "id": "6ZabgOxHhJSt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick Observation based on twitter search box by number\n",
        "\n",
        "# Quite Few (fewer than 50):\n",
        "# 'testimoni scarlett whitening'\n",
        "# 'scarlett whitening testimoni'\n",
        "\n",
        "# Pretty Much:\n",
        "# 'scarlett whitening review'\n",
        "# 'review scarlett whitening'\n",
        "# 'scarlett whitening'\n",
        "\n",
        "# Not reccomended :\n",
        "# 'scarlett review', Bcz the result is irrelevant\n",
        "search_words = ['scarlett whitening review']"
      ],
      "metadata": {
        "id": "4zs69ewilZFX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "today = datetime.today().strftime(\"%Y-%m-%d\")\n",
        "last_week = datetime.today() - timedelta(7)\n",
        "last_week = last_week.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "search_words = ['#scarlettwhiteningreview OR #reviewscarlettwhitening OR #scarlettwhitening']\n",
        "date_since = last_week\n",
        "date_until = today"
      ],
      "metadata": {
        "id": "JTKRmrOjlxtd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Set Up"
      ],
      "metadata": {
        "id": "bccZ9r4N37V9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OAuthKeys = {'consumer_key':APIKey, 'consumer_secret':APISecretKey, 'access_token_key':AccessToken, 'access_token_secret':AccessTokenSecret}"
      ],
      "metadata": {
        "id": "GoxwPSx4Z4Jp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auth = tweepy.OAuthHandler(OAuthKeys['consumer_key'], OAuthKeys['consumer_secret'])"
      ],
      "metadata": {
        "id": "-RFS6N3dbDwh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "API = tweepy.API(auth)"
      ],
      "metadata": {
        "id": "33sF2nx4fOAx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scraptweets(search_words, date_since, date_until):\n",
        "\n",
        "    db_tweets = pd.DataFrame(columns=['username', 'tweetcreatedts', 'text'])\n",
        "\n",
        "    tweets = tweepy.Cursor(\n",
        "                    API.search, q=search_words, lang=\"id\", \n",
        "                    since=date_since, until=date_until,  tweet_mode='extended').items(1000)\n",
        "\n",
        "    tweet_list = [tweet for tweet in tweets]\n",
        "\n",
        "    for tweet in tweet_list:\n",
        "        username = tweet.user.screen_name\n",
        "        tweetcreatedts = tweet.created_at\n",
        "\n",
        "        try:\n",
        "            text = tweet.retweeted_status.full_text\n",
        "        except AttributeError:\n",
        "            text = tweet.full_text\n",
        "\n",
        "        ith_tweet = [username, tweetcreatedts, text]\n",
        "\n",
        "        db_tweets.loc[len(db_tweets)] = ith_tweet\n",
        "    \n",
        "    print('Proses Scrapping Selesai Dengan Jumlah Data', len(db_tweets))\n",
        "    filename = 'reviews.csv'\n",
        "    db_tweets.to_csv(filename, index=False)"
      ],
      "metadata": {
        "id": "d95nfl_zlZ5v"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scraptweets(search_words, date_since, date_until)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "bW8pWiP8ocZV",
        "outputId": "a8ebab4a-70cd-4968-cb33-de49bb412f75"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TweepError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTweepError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-38bd14f59402>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscraptweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_since\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_until\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-eb9998475871>\u001b[0m in \u001b[0;36mscraptweets\u001b[0;34m(search_words, date_since, date_until)\u001b[0m\n\u001b[1;32m      7\u001b[0m                     since=date_since, until=date_until,  tweet_mode='extended').items(1000)\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtweet_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtweet\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweet_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-eb9998475871>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m                     since=date_since, until=date_until,  tweet_mode='extended').items(1000)\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtweet_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtweet\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweet_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;31m# Reached end of current page, get the next page...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRawParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__self__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/binder.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/binder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mRateLimitError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mTweepError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_error_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;31m# Parse the response payload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTweepError\u001b[0m: Twitter error response: status code = 403"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def toDataFrame(tweets):\n",
        "  data = pd.DataFrame\n",
        "  data.['tweetID'] = [tweet.id for tweet in tweets]\n",
        "  data.['tweetText'] = [tweet.text.encode('utf-8') for tweet in tweets]\n",
        "  data.['tweetCreated'] = [tweet.created_at for tweet in tweets]\n",
        "  data.['userID'] = [tweet.user.id for tweet in tweets]\n",
        "  data.['username'] = [tweet.user.name for tweet in tweets]\n",
        "  return data\n",
        "\n",
        "data = toDataFrame(results)\n",
        "data.to_csv('reviews.csv', index=False)"
      ],
      "metadata": {
        "id": "NNlPp6EcqPdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Clean Tweets"
      ],
      "metadata": {
        "id": "V6JVZ4B_g9yT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_tweet(self, tweet): \n",
        "    ''' \n",
        "    Use sumple regex statemnents to clean tweet text by removing links and special characters\n",
        "    '''\n",
        "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) \\\n",
        "                                |(\\w+:\\/\\/\\S+)\", \" \", tweet).split()) \n",
        "def deEmojify(text):\n",
        "    '''\n",
        "    Strip all non-ASCII characters to remove emoji characters\n",
        "    '''\n",
        "    if text:\n",
        "        return text.encode('ascii', 'ignore').decode('ascii')\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "2JDeNVZ8ggBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['tweetText'].clean_tweet\n",
        "data['tweetText'].deEmojify"
      ],
      "metadata": {
        "id": "jvuqs9KSilOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Spark Session"
      ],
      "metadata": {
        "id": "_W9uHiTLman5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#memulai Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "  .master(\"local[2]\") \\\n",
        "  .appName(\"Proyek_Sentiment Analysis\") \\\n",
        "  .getOrCreate()\n"
      ],
      "metadata": {
        "id": "OhsszDzzG0Qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# melakukan load dataset menggunakan library pandas (dikarenakan Spark load data tidak sesuai)\n",
        "clean_data = data\n",
        "# melakukan load dataset kedalam Spark dan menampilkannya\n",
        "reviews = spark.createDataFrame(clean_data)\n",
        "reviews.show(10)\n",
        "# melakukan Konversi Data ke format Parquet\n",
        "reviews.write.format(\"parquet\").mode(\"overwrite\").save(\"D:/semester8/PDB/Proyek/Data_Parquet\")\n"
      ],
      "metadata": {
        "id": "LgH2NrEyHEMR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}